{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd5cf92a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu in c:\\anaconda3\\lib\\site-packages (2.10.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.1.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.47.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.20.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (22.12.6)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.10.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (52.0.0.post20210125)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (14.0.6)\n",
      "Requirement already satisfied: packaging in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (20.9)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.29.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.36.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (2.25.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (2.15.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (1.8.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-gpu) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow-gpu) (5.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow-gpu) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (2022.12.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-gpu) (3.2.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\anaconda3\\lib\\site-packages (from packaging->tensorflow-gpu) (2.4.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b547310",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Using cached huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: filelock in c:\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\anaconda3\\lib\\site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e9c16ef3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastai\n",
      "  Downloading fastai-2.7.10-py3-none-any.whl (240 kB)\n",
      "Collecting fastcore<1.6,>=1.4.5\n",
      "  Downloading fastcore-1.5.27-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\anaconda3\\lib\\site-packages (from fastai) (5.4.1)\n",
      "Collecting fastdownload<2,>=0.0.5\n",
      "  Downloading fastdownload-0.0.7-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda3\\lib\\site-packages (from fastai) (0.24.1)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in c:\\anaconda3\\lib\\site-packages (from fastai) (0.14.0)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from fastai) (2.25.1)\n",
      "Collecting spacy<4\n",
      "  Downloading spacy-3.4.4-cp38-cp38-win_amd64.whl (12.2 MB)\n",
      "Requirement already satisfied: pillow>6.0.0 in c:\\anaconda3\\lib\\site-packages (from fastai) (8.2.0)\n",
      "Requirement already satisfied: packaging in c:\\anaconda3\\lib\\site-packages (from fastai) (20.9)\n",
      "Requirement already satisfied: torch<1.14,>=1.7 in c:\\anaconda3\\lib\\site-packages (from fastai) (1.13.0)\n",
      "Requirement already satisfied: scipy in c:\\anaconda3\\lib\\site-packages (from fastai) (1.9.3)\n",
      "Requirement already satisfied: pip in c:\\anaconda3\\lib\\site-packages (from fastai) (21.0.1)\n",
      "Requirement already satisfied: pandas in c:\\anaconda3\\lib\\site-packages (from fastai) (1.2.4)\n",
      "Requirement already satisfied: matplotlib in c:\\anaconda3\\lib\\site-packages (from fastai) (3.3.4)\n",
      "Collecting fastprogress>=0.2.4\n",
      "  Downloading fastprogress-1.0.3-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (4.59.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (1.20.1)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp38-cp38-win_amd64.whl (96 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp38-cp38-win_amd64.whl (30 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.10\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.5-cp38-cp38-win_amd64.whl (481 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (1.10.2)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (2.11.3)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.6-cp38-cp38-win_amd64.whl (1.3 MB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp38-cp38-win_amd64.whl (18 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (52.0.0.post20210125)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\anaconda3\\lib\\site-packages (from packaging->fastai) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<4->fastai) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests->fastai) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests->fastai) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests->fastai) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests->fastai) (1.26.4)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp38-cp38-win_amd64.whl (7.0 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.3-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<4->fastai) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\anaconda3\\lib\\site-packages (from jinja2->spacy<4->fastai) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (2.8.1)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->fastai) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\anaconda3\\lib\\site-packages (from pandas->fastai) (2021.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn->fastai) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\anaconda3\\lib\\site-packages (from scikit-learn->fastai) (1.0.1)\n",
      "Installing collected packages: catalogue, srsly, murmurhash, cymem, wasabi, typer, smart-open, preshed, confection, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, fastprogress, fastcore, spacy, fastdownload, fastai\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.3 cymem-2.0.7 fastai-2.7.10 fastcore-1.5.27 fastdownload-0.0.7 fastprogress-1.0.3 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 smart-open-6.3.0 spacy-3.4.4 spacy-legacy-3.0.10 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.6 typer-0.7.0 wasabi-0.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4b787790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\–ú–∞–∫—Å–∏–º\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,  CSVLogger\n",
    "from keras.layers import Add, Dense, Input, LSTM\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_scheduler\n",
    "from transformers import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('cmudict')\n",
    "from nltk.corpus import cmudict\n",
    "import torch\n",
    "import random\n",
    "from fastai.text.all import *\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "283dab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2151190a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1377d9d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"DatasetFinal.xlsx\")\n",
    "df=df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2604776e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FirstRow</th>\n",
       "      <th>SecondRow</th>\n",
       "      <th>ThirdRow</th>\n",
       "      <th>Original</th>\n",
       "      <th>FirstKeyPhrase</th>\n",
       "      <th>SecondKeyPhrase</th>\n",
       "      <th>KeyEmotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>delicate savage</td>\n",
       "      <td>you'll never hold the cinder</td>\n",
       "      <td>but still you will burn</td>\n",
       "      <td>delicate savage   you'll never hold the cinder   but still you will burn</td>\n",
       "      <td>never hold</td>\n",
       "      <td>delicate savage</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our destination</td>\n",
       "      <td>the skyline of this city</td>\n",
       "      <td>shining horizon</td>\n",
       "      <td>our destination   the skyline of this city   shining horizon</td>\n",
       "      <td>city shining horizon</td>\n",
       "      <td>skyline</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a splash and a cry</td>\n",
       "      <td>words pulled from the riverside</td>\n",
       "      <td>dried in the hot sun</td>\n",
       "      <td>a splash and a cry    words pulled from the riverside    dried in the hot sun</td>\n",
       "      <td>cry words pulled</td>\n",
       "      <td>riverside dried</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hurt but poised for war</td>\n",
       "      <td>sturdy in crestfallen slumps</td>\n",
       "      <td>warrior spirit</td>\n",
       "      <td>hurt but poised for war   sturdy in crestfallen slumps   warrior spirit</td>\n",
       "      <td>crestfallen slumps warrior spirit</td>\n",
       "      <td>war sturdy</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>steamy mist rising</td>\n",
       "      <td>rocks receiving downward crash</td>\n",
       "      <td>as the jungle weeps</td>\n",
       "      <td>steamy mist rising   rocks receiving downward crash   as the jungle weeps</td>\n",
       "      <td>steamy mist rising rocks receiving downward crash</td>\n",
       "      <td>jungle weeps</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13666</th>\n",
       "      <td>finding someone who</td>\n",
       "      <td>has pure intentions and heart</td>\n",
       "      <td>is so difficult</td>\n",
       "      <td>finding someone who has pure intentions and heart is so difficult</td>\n",
       "      <td>pure intentions</td>\n",
       "      <td>finding someone</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13667</th>\n",
       "      <td>everything changes</td>\n",
       "      <td>in a split second so just</td>\n",
       "      <td>enjoy the moment</td>\n",
       "      <td>everything changes in a split second so just enjoy the moment</td>\n",
       "      <td>split second</td>\n",
       "      <td>everything changes</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13668</th>\n",
       "      <td>just because you could</td>\n",
       "      <td>spell your child's name a weird way</td>\n",
       "      <td>doesn't mean you should</td>\n",
       "      <td>just because you could spell your child's name a weird way doesn't mean you should</td>\n",
       "      <td>weird way</td>\n",
       "      <td>could spell</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13669</th>\n",
       "      <td>i feel like alex</td>\n",
       "      <td>kate is gonna admit their</td>\n",
       "      <td>true identity</td>\n",
       "      <td>i feel like alex kate is gonna admit their true identity</td>\n",
       "      <td>feel like alex kate</td>\n",
       "      <td>true identity</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13670</th>\n",
       "      <td>sometimes the comment</td>\n",
       "      <td>section is funnier than</td>\n",
       "      <td>the actual tweet</td>\n",
       "      <td>sometimes the comment section is funnier than the actual tweet</td>\n",
       "      <td>comment section</td>\n",
       "      <td>actual tweet</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13671 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      FirstRow                            SecondRow  \\\n",
       "0              delicate savage         you'll never hold the cinder   \n",
       "1              our destination             the skyline of this city   \n",
       "2           a splash and a cry      words pulled from the riverside   \n",
       "3      hurt but poised for war         sturdy in crestfallen slumps   \n",
       "4           steamy mist rising       rocks receiving downward crash   \n",
       "...                        ...                                  ...   \n",
       "13666      finding someone who        has pure intentions and heart   \n",
       "13667       everything changes            in a split second so just   \n",
       "13668   just because you could  spell your child's name a weird way   \n",
       "13669         i feel like alex            kate is gonna admit their   \n",
       "13670    sometimes the comment              section is funnier than   \n",
       "\n",
       "                      ThirdRow  \\\n",
       "0      but still you will burn   \n",
       "1              shining horizon   \n",
       "2         dried in the hot sun   \n",
       "3               warrior spirit   \n",
       "4          as the jungle weeps   \n",
       "...                        ...   \n",
       "13666          is so difficult   \n",
       "13667         enjoy the moment   \n",
       "13668  doesn't mean you should   \n",
       "13669            true identity   \n",
       "13670         the actual tweet   \n",
       "\n",
       "                                                                                 Original  \\\n",
       "0                delicate savage   you'll never hold the cinder   but still you will burn   \n",
       "1                            our destination   the skyline of this city   shining horizon   \n",
       "2           a splash and a cry    words pulled from the riverside    dried in the hot sun   \n",
       "3                 hurt but poised for war   sturdy in crestfallen slumps   warrior spirit   \n",
       "4               steamy mist rising   rocks receiving downward crash   as the jungle weeps   \n",
       "...                                                                                   ...   \n",
       "13666                   finding someone who has pure intentions and heart is so difficult   \n",
       "13667                       everything changes in a split second so just enjoy the moment   \n",
       "13668  just because you could spell your child's name a weird way doesn't mean you should   \n",
       "13669                            i feel like alex kate is gonna admit their true identity   \n",
       "13670                      sometimes the comment section is funnier than the actual tweet   \n",
       "\n",
       "                                          FirstKeyPhrase     SecondKeyPhrase  \\\n",
       "0                                             never hold     delicate savage   \n",
       "1                                   city shining horizon             skyline   \n",
       "2                                       cry words pulled     riverside dried   \n",
       "3                      crestfallen slumps warrior spirit          war sturdy   \n",
       "4      steamy mist rising rocks receiving downward crash        jungle weeps   \n",
       "...                                                  ...                 ...   \n",
       "13666                                    pure intentions     finding someone   \n",
       "13667                                       split second  everything changes   \n",
       "13668                                          weird way         could spell   \n",
       "13669                                feel like alex kate       true identity   \n",
       "13670                                    comment section        actual tweet   \n",
       "\n",
       "      KeyEmotion  \n",
       "0       POSITIVE  \n",
       "1       POSITIVE  \n",
       "2       POSITIVE  \n",
       "3       NEGATIVE  \n",
       "4       NEGATIVE  \n",
       "...          ...  \n",
       "13666   NEGATIVE  \n",
       "13667   POSITIVE  \n",
       "13668   NEGATIVE  \n",
       "13669   POSITIVE  \n",
       "13670   POSITIVE  \n",
       "\n",
       "[13671 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d14231a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=df[[\"Original\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "177a8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "newY=[]\n",
    "for i in range(0,len(X)):\n",
    "    newY.append(Y.iloc[i]['Original'])\n",
    "Y=newY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "65afa1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('haiku.txt', 'w',encoding=\"utf-8\") as outfile:\n",
    "    for i in Y:\n",
    "        outfile.write(\"%s\\n\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e6a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#–°–¥–µ—Å—å –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "eb12e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator\n",
    "\n",
    "\n",
    "def train(train_file_path,model_name,\n",
    "          output_dir,\n",
    "          overwrite_output_dir,\n",
    "          per_device_train_batch_size,\n",
    "          num_train_epochs,\n",
    "          save_steps):\n",
    "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "  data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "  tokenizer.save_pretrained(output_dir)\n",
    "      \n",
    "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "  model.save_pretrained(output_dir)\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "          output_dir=output_dir,\n",
    "          overwrite_output_dir=overwrite_output_dir,\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "      )\n",
    "\n",
    "  trainer = Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          data_collator=data_collator,\n",
    "          train_dataset=train_dataset,\n",
    "  )\n",
    "      \n",
    "  trainer.train()\n",
    "  trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "05081e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"haiku.txt\"\n",
    "model_name = 'gpt2'\n",
    "output_dir = 'AAAModelresultFinal'\n",
    "overwrite_output_dir = False\n",
    "per_device_train_batch_size = 20\n",
    "num_train_epochs = 25.0\n",
    "save_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "d3432d47",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at C:\\Users\\–ú–∞–∫—Å–∏–º/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\e7da7f221d5bf496a48136c0cd264e630fe9fcc8\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\–ú–∞–∫—Å–∏–º/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\e7da7f221d5bf496a48136c0cd264e630fe9fcc8\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\–ú–∞–∫—Å–∏–º/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\e7da7f221d5bf496a48136c0cd264e630fe9fcc8\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "C:\\anaconda3\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Loading features from cached file cached_lm_GPT2Tokenizer_128_haiku.txt [took 0.011 s]\n",
      "tokenizer config file saved in AAAModelresultFinal\\tokenizer_config.json\n",
      "Special tokens file saved in AAAModelresultFinal\\special_tokens_map.json\n",
      "loading configuration file config.json from cache at C:\\Users\\–ú–∞–∫—Å–∏–º/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\e7da7f221d5bf496a48136c0cd264e630fe9fcc8\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\–ú–∞–∫—Å–∏–º/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\e7da7f221d5bf496a48136c0cd264e630fe9fcc8\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Configuration saved in AAAModelresultFinal\\config.json\n",
      "Model weights saved in AAAModelresultFinal\\pytorch_model.bin\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "C:\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2006\n",
      "  Num Epochs = 25\n",
      "  Instantaneous batch size per device = 20\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 20\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2525\n",
      "  Number of trainable parameters = 124439808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2525' max='2525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2525/2525 8:59:57, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.843500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.374200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.870400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.758100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to AAAModelresultFinal\\checkpoint-500\n",
      "Configuration saved in AAAModelresultFinal\\checkpoint-500\\config.json\n",
      "Model weights saved in AAAModelresultFinal\\checkpoint-500\\pytorch_model.bin\n",
      "Saving model checkpoint to AAAModelresultFinal\\checkpoint-1000\n",
      "Configuration saved in AAAModelresultFinal\\checkpoint-1000\\config.json\n",
      "Model weights saved in AAAModelresultFinal\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to AAAModelresultFinal\\checkpoint-1500\n",
      "Configuration saved in AAAModelresultFinal\\checkpoint-1500\\config.json\n",
      "Model weights saved in AAAModelresultFinal\\checkpoint-1500\\pytorch_model.bin\n",
      "Saving model checkpoint to AAAModelresultFinal\\checkpoint-2000\n",
      "Configuration saved in AAAModelresultFinal\\checkpoint-2000\\config.json\n",
      "Model weights saved in AAAModelresultFinal\\checkpoint-2000\\pytorch_model.bin\n",
      "Saving model checkpoint to AAAModelresultFinal\\checkpoint-2500\n",
      "Configuration saved in AAAModelresultFinal\\checkpoint-2500\\config.json\n",
      "Model weights saved in AAAModelresultFinal\\checkpoint-2500\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to AAAModelresultFinal\n",
      "Configuration saved in AAAModelresultFinal\\config.json\n",
      "Model weights saved in AAAModelresultFinal\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
